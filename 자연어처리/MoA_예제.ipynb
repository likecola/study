{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from rich.console import Console\n",
    "\n",
    "console = Console()\n",
    "\n",
    "# 예제 모델 경로를 정의합니다.\n",
    "proposer_model_paths = [\n",
    "    '/path/to/first-model',\n",
    "    '/path/to/second-model',\n",
    "    '/path/to/third-model'\n",
    "]\n",
    "\n",
    "aggregator_model_path = '/path/to/aggregator-model'  # 집계 모델 경로\n",
    "\n",
    "def get_least_used_gpu():\n",
    "    \"\"\"가장 적게 사용 중인 GPU를 반환하는 함수\"\"\"\n",
    "    device_count = torch.cuda.device_count()\n",
    "    if device_count == 0:\n",
    "        raise RuntimeError(\"No CUDA devices found.\")\n",
    "    \n",
    "    gpu_memories = [torch.cuda.memory_allocated(i) for i in range(device_count)]\n",
    "    least_used_gpu = gpu_memories.index(min(gpu_memories))\n",
    "    \n",
    "    return torch.device(f\"cuda:{least_used_gpu}\")\n",
    "\n",
    "def load_model_and_tokenizer(model_path):\n",
    "    try:\n",
    "        model = AutoModelForCausalLM.from_pretrained(model_path, trust_remote_code=True)\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)\n",
    "        \n",
    "        # 모델이 DataParallel을 사용하고 있는지 확인하고, 그렇다면 model.module으로 접근\n",
    "        if torch.cuda.device_count() > 1:\n",
    "            model = torch.nn.DataParallel(model)\n",
    "        \n",
    "        return model, tokenizer\n",
    "    except Exception as e:\n",
    "        console.log(f\"모델 로딩 중 오류 발생: {e}\", style=\"bold red\")\n",
    "        raise\n",
    "\n",
    "def unload_model_and_tokenizer(model, tokenizer):\n",
    "    del model\n",
    "    del tokenizer\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "def generate_proposals(prompt, model_paths):\n",
    "    proposals = []\n",
    "    \n",
    "    for path in model_paths:\n",
    "        console.log(f\"Loading model from {path}\")\n",
    "        model, tokenizer = load_model_and_tokenizer(path)\n",
    "        device = get_least_used_gpu()\n",
    "        model.to(device)\n",
    "        \n",
    "        # 모델별 max_length 설정\n",
    "        if \"first-model\" in path:\n",
    "            max_length = 300\n",
    "        elif \"second-model\" in path:\n",
    "            max_length = 2048\n",
    "        else:\n",
    "            max_length = 4096  # 다른 모델의 경우\n",
    "        \n",
    "        console.log(f\"Using max_length: {max_length}\")\n",
    "        try:\n",
    "            inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, padding=\"max_length\", max_length=max_length).to(device)\n",
    "            console.log(f\"Inputs: {inputs['input_ids'].shape}\")  # 디버깅을 위한 입력 크기 확인\n",
    "            with torch.no_grad():\n",
    "                outputs = model.module.generate(  # DataParallel 사용 시 model.module을 통해 접근\n",
    "                    **inputs,\n",
    "                    max_new_tokens=100,\n",
    "                    num_return_sequences=1,\n",
    "                    num_beams=3,\n",
    "                    do_sample=True,\n",
    "                    top_k=50,\n",
    "                    temperature=0.7\n",
    "                )\n",
    "            responses = [tokenizer.decode(output, skip_special_tokens=True).strip() for output in outputs]\n",
    "            console.log(f\"Outputs: {[o.shape for o in outputs]}\")  # 디버깅을 위한 출력 크기 확인\n",
    "            proposals.extend(responses)\n",
    "        except Exception as e:\n",
    "            console.log(f\"모델 생성 중 오류 발생: {e}\", style=\"bold red\")\n",
    "        finally:\n",
    "            unload_model_and_tokenizer(model, tokenizer)\n",
    "    \n",
    "    cleaned_proposals = list(set(proposals))  # 중복 제거\n",
    "    return cleaned_proposals\n",
    "\n",
    "def aggregate_responses(proposals, model_path, original_prompt):\n",
    "    try:\n",
    "        aggregator_device = get_least_used_gpu()\n",
    "        model, tokenizer = load_model_and_tokenizer(model_path)\n",
    "        model.to(aggregator_device)\n",
    "        \n",
    "        aggregator_prompt = (\n",
    "            f\"다양한 AI 모델들이 '{original_prompt}' 질문에 대해 생성한 응답들을 종합하여 일관되고 포괄적인 답변을 제공하세요:\\n\"\n",
    "            f\"{' '.join(proposals)[:1000]}\"  # 입력 길이 제한을 더 줄임\n",
    "        )\n",
    "        inputs = tokenizer(aggregator_prompt, return_tensors=\"pt\").to(aggregator_device)\n",
    "        with torch.no_grad():\n",
    "            outputs = model.module.generate(  # DataParallel 사용 시 model.module을 통해 접근\n",
    "                **inputs,\n",
    "                max_new_tokens=300\n",
    "            )\n",
    "        final_response = tokenizer.decode(outputs[0], skip_special_tokens=True).strip()\n",
    "    except Exception as e:\n",
    "        console.log(f\"집계 모델 생성 중 오류 발생: {e}\", style=\"bold red\")\n",
    "        final_response = \"\"\n",
    "    finally:\n",
    "        try:\n",
    "            unload_model_and_tokenizer(model, tokenizer)\n",
    "        except UnboundLocalError:\n",
    "            console.log(\"모델 또는 토크나이저를 언로드하는 중 오류 발생\", style=\"bold red\")\n",
    "    \n",
    "    return final_response\n",
    "\n",
    "def main():\n",
    "    while True:\n",
    "        question = input(\"질문을 입력하세요 (종료하려면 'exit' 입력): \")\n",
    "        if question.lower() == 'exit':\n",
    "            break\n",
    "\n",
    "        console.log(\"Generating proposals for the question\")\n",
    "        proposals = generate_proposals(question, proposer_model_paths)\n",
    "        console.log(f\"Proposals: {proposals}\")\n",
    "\n",
    "        console.log(\"Aggregating responses\")\n",
    "        final_response = aggregate_responses(proposals, aggregator_model_path, question)\n",
    "        console.print(\"제안자 모델의 응답들:\", proposals)\n",
    "        console.print(\"최종 집계된 응답:\", final_response, style=\"bold green\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 코드 설명\n",
    "\n",
    "이 코드는 여러 개의 제안자 모델(Proposer Models)과 하나의 집계 모델(Aggregator Model)을 사용하여 사용자로부터 받은 질문에 대한 응답을 생성하고, 이를 종합하여 최종 답변을 생성하는 프로그램입니다.\n",
    "\n",
    "#### 주요 기능\n",
    "1. **get_least_used_gpu():**\n",
    "   - 현재 사용 가능한 GPU 중 가장 적게 사용되고 있는 GPU를 선택하여 반환합니다. 이를 통해 모델을 로드하고 계산을 수행할 GPU를 동적으로 선택합니다.\n",
    "\n",
    "2. **load_model_and_tokenizer(model_path):**\n",
    "   - 주어진 경로에서 모델과 토크나이저를 로드합니다. 여러 GPU를 사용하는 경우 `DataParallel`을 적용하여 모델을 병렬 처리합니다.\n",
    "\n",
    "3. **unload_model_and_tokenizer(model, tokenizer):**\n",
    "   - 사용한 모델과 토크나이저를 메모리에서 해제하고 GPU 캐시를 비웁니다.\n",
    "\n",
    "4. **generate_proposals(prompt, model_paths):**\n",
    "   - 여러 제안자 모델을 사용하여 입력된 프롬프트에 대한 응답을 생성합니다. 각 모델은 적절한 GPU에 할당되며, 모델별로 설정된 최대 길이(`max_length`)를 기준으로 입력을 처리합니다.\n",
    "   - 응답은 중복을 제거하여 반환됩니다.\n",
    "\n",
    "5. **aggregate_responses(proposals, model_path, original_prompt):**\n",
    "   - 집계 모델을 사용하여 제안자 모델들로부터 생성된 응답을 종합하고 최종 답변을 생성합니다.\n",
    "   - 입력된 응답들을 하나의 프롬프트로 결합하여 집계 모델에 입력하고, 최종 결과를 반환합니다.\n",
    "\n",
    "6. **main():**\n",
    "   - 사용자로부터 질문을 입력받고, 제안자 모델을 통해 응답을 생성한 후, 집계 모델을 사용하여 최종 답변을 생성합니다.\n",
    "   - 프로그램은 사용자가 \"exit\"을 입력할 때까지 반복됩니다.\n",
    "\n",
    "이 프로그램은 복잡한 모델 간의 협업을 통해 더 일관되고 포괄적인 답변을 생성하는 데 중점을 두고 있으며, 다중 GPU 환경에서의 효율적인 리소스 사용을 고려한 설계가 특징입니다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
