{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NsPhHN8cLiNy"
      },
      "source": [
        "#  모델 구성 및 학습 (예제) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mwugLo4CLAsI",
        "outputId": "13e392f8-1118-4f94-db65-ed5f64abbafb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - accuracy: 0.4500 - loss: 4.6189\n",
            "Epoch 2/10\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.4500 - loss: 4.5431  \n",
            "Epoch 3/10\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.4500 - loss: 4.4734 \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/lib/python3.10/contextlib.py:153: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
            "  self.gen.throw(typ, value, traceback)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 4/10\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.4500 - loss: 4.4020 \n",
            "Epoch 5/10\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.4500 - loss: 4.3292 \n",
            "Epoch 6/10\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.4500 - loss: 4.2552\n",
            "Epoch 7/10\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.4500 - loss: 4.1804 \n",
            "Epoch 8/10\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.4500 - loss: 4.1051 \n",
            "Epoch 9/10\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.4500 - loss: 4.0295 \n",
            "Epoch 10/10\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.4500 - loss: 3.9540 \n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7aa39048e9b0>"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import os\n",
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "\n",
        "# 예제용 CSV 파일 생성\n",
        "csv_file = 'sample_data.csv'\n",
        "df = pd.DataFrame({\n",
        "    'feature1': [1, 2, 3, 4, 5],\n",
        "    'feature2': [10, 20, 30, 40, 50],\n",
        "    'label': [0, 1, 0, 1, 0]\n",
        "})\n",
        "df.to_csv(csv_file, index=False)\n",
        "\n",
        "# CSV 파일 미리보기\n",
        "df.head()\n",
        "\n",
        "# CSV 데이터셋 로드\n",
        "def parse_csv_line(line):\n",
        "    # CSV 파일의 필드 정의\n",
        "    defaults = [tf.constant([], dtype=tf.float32),  # feature1\n",
        "                tf.constant([], dtype=tf.float32),  # feature2\n",
        "                tf.constant([], dtype=tf.int32)]    # label\n",
        "\n",
        "    parsed_line = tf.io.decode_csv(line, record_defaults=defaults)\n",
        "    features = tf.stack(parsed_line[:-1], axis=-1)  # features를 하나의 tensor로 스택\n",
        "    label = parsed_line[-1]\n",
        "    return features, label\n",
        "\n",
        "# CSV 파일에서 데이터셋 생성\n",
        "dataset = tf.data.TextLineDataset(csv_file)\n",
        "dataset = dataset.skip(1)  # 헤더 스킵\n",
        "dataset = dataset.map(parse_csv_line)  # CSV 파싱\n",
        "\n",
        "# 데이터셋을 배치로 묶기\n",
        "dataset = dataset.batch(2)\n",
        "\n",
        "# 간단한 모델 구성\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Dense(10, activation='relu', input_shape=(2,)),\n",
        "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "# 모델 컴파일\n",
        "model.compile(optimizer='adam',\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# 모델 학습\n",
        "model.fit(dataset, epochs=10)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_RO5eq6yL8YD",
        "outputId": "79e992d6-c175-40ed-f2cb-4a8c3158d85b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.4500 - loss: 3.9071\n",
            "Loss: 5.514544486999512, Accuracy: 0.4000000059604645\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 140ms/step\n",
            "Predicted: [[0.954]\n",
            " [0.998]], Actual: [0 1]\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
            "Predicted: [[1.]\n",
            " [1.]], Actual: [0 1]\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 93ms/step\n",
            "Predicted: [[1.]], Actual: [0]\n"
          ]
        }
      ],
      "source": [
        "# 모델 평가\n",
        "loss, accuracy = model.evaluate(dataset)\n",
        "print(f'Loss: {loss}, Accuracy: {accuracy}')\n",
        "\n",
        "# 예측 수행\n",
        "for features, label in dataset.take(5):\n",
        "    prediction = model.predict(features)\n",
        "    print(f'Predicted: {prediction}, Actual: {label}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DweYe9FcbMK_"
      },
      "source": [
        "##### Copyright 2019 The TensorFlow Authors.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "AVV2e0XKbJeX"
      },
      "outputs": [],
      "source": [
        "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sUtoed20cRJJ"
      },
      "source": [
        "# CSV 데이터 로드하기"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ap_W4aQcgNT"
      },
      "source": [
        "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
        "  <td><a target=\"_blank\" href=\"https://www.tensorflow.org/tutorials/load_data/csv\"><img src=\"https://www.tensorflow.org/images/tf_logo_32px.png\">TensorFlow.org에서 보기</a></td>\n",
        "  <td><a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/docs-l10n/blob/master/site/ko/tutorials/load_data/csv.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\">Google Colab에서 실행하기</a></td>\n",
        "  <td><a target=\"_blank\" href=\"https://github.com/tensorflow/docs-l10n/blob/master/site/ko/tutorials/load_data/csv.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\">GitHub에서 소스 보기</a></td>\n",
        "  <td><a href=\"https://storage.googleapis.com/tensorflow_docs/docs-l10n/site/ko/tutorials/load_data/csv.ipynb\"><img src=\"https://www.tensorflow.org/images/download_logo_32px.png\">노트북 다운로드하기</a></td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C-3Xbt0FfGfs"
      },
      "source": [
        "이 튜토리얼은 TensorFlow에서 CSV 데이터를 사용하는 방법에 대한 예제를 제공합니다.\n",
        "\n",
        "다음과 같이 두 가지 주요 내용이 있습니다.\n",
        "\n",
        "1. **Loading the data off disk**\n",
        "2. **Pre-processing it into a form suitable for training.**\n",
        "\n",
        "이 튜토리얼은 로딩에 중점을 두며 전처리에 대한 몇 가지 빠른 예를 제공합니다. 전처리 측면에 대해 자세히 알아보려면 [전처리 레이어 작업](https://www.tensorflow.org/guide/keras/preprocessing_layers) 가이드 및 [Keras 전처리 레이어를 사용하여 구조화된 데이터 분류](../structured_data/preprocessing_layers.ipynb) 튜토리얼을 확인하세요.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fgZ9gjmPfSnK"
      },
      "source": [
        "## 설정하기"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "baYFZMW_bJHh"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Make numpy values easier to read.\n",
        "np.set_printoptions(precision=3, suppress=True)\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "# np.set_printoptions(precision=3, suppress=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ZhJYbJxHNGJ"
      },
      "source": [
        "## 인메모리 데이터"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ny5TEgcmHjVx"
      },
      "source": [
        "작은 크기의 CSV 데이터세트를 사용하여 TensorFlow 모델을 훈련시키는 가장 간단한 방법은 이를 메모리에 pandas Dataframe 또는 NumPy 배열로 로드하는 것입니다.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LgpBOuU8PGFf"
      },
      "source": [
        "비교적 간단한 예는 [전복 데이터세트](https://archive.ics.uci.edu/ml/datasets/abalone)입니다.\n",
        "\n",
        "- 데이터세트의 크기가 작습니다.\n",
        "- 모든 입력 특성은 모두 제한된 범위의 부동 소수점 값입니다.\n",
        "\n",
        "다음은 [Pandas `DataFrame`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.html)에 데이터를 다운로드하는 방법입니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "IZVExo9DKoNz",
        "outputId": "f91a4f1d-64ea-4bd1-94e5-0104365ba098"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "summary": "{\n  \"name\": \"abalone_train\",\n  \"rows\": 5,\n  \"fields\": [\n    {\n      \"column\": \"Length\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.12141869707750942,\n        \"min\": 0.395,\n        \"max\": 0.7,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          0.7,\n          0.45,\n          0.53\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Diameter\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.08555699854482975,\n        \"min\": 0.315,\n        \"max\": 0.525,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          0.525,\n          0.355,\n          0.42\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Height\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.03774917217635375,\n        \"min\": 0.105,\n        \"max\": 0.19,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          0.19,\n          0.12,\n          0.13\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Whole weight\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.502557011691211,\n        \"min\": 0.3515,\n        \"max\": 1.6015,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          1.6015,\n          0.412,\n          0.8365\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Shucked weight\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.24428124160483544,\n        \"min\": 0.1145,\n        \"max\": 0.707,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          0.707,\n          0.1145,\n          0.3745\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Viscera weight\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.11906279855605612,\n        \"min\": 0.0665,\n        \"max\": 0.365,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          0.365,\n          0.0665,\n          0.167\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Shell weight\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.12201823634195012,\n        \"min\": 0.1195,\n        \"max\": 0.43,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          0.43,\n          0.16,\n          0.249\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Age\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 3,\n        \"min\": 10,\n        \"max\": 19,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          10,\n          19,\n          11\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}",
              "type": "dataframe"
            },
            "text/html": [
              "\n",
              "  <div id=\"df-15e0d9ef-22c4-42da-8597-8aaa98742188\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Length</th>\n",
              "      <th>Diameter</th>\n",
              "      <th>Height</th>\n",
              "      <th>Whole weight</th>\n",
              "      <th>Shucked weight</th>\n",
              "      <th>Viscera weight</th>\n",
              "      <th>Shell weight</th>\n",
              "      <th>Age</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>3315</th>\n",
              "      <td>0.605</td>\n",
              "      <td>0.475</td>\n",
              "      <td>0.180</td>\n",
              "      <td>0.9365</td>\n",
              "      <td>0.3940</td>\n",
              "      <td>0.2190</td>\n",
              "      <td>0.2950</td>\n",
              "      <td>15</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3316</th>\n",
              "      <td>0.700</td>\n",
              "      <td>0.525</td>\n",
              "      <td>0.190</td>\n",
              "      <td>1.6015</td>\n",
              "      <td>0.7070</td>\n",
              "      <td>0.3650</td>\n",
              "      <td>0.4300</td>\n",
              "      <td>10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3317</th>\n",
              "      <td>0.530</td>\n",
              "      <td>0.420</td>\n",
              "      <td>0.130</td>\n",
              "      <td>0.8365</td>\n",
              "      <td>0.3745</td>\n",
              "      <td>0.1670</td>\n",
              "      <td>0.2490</td>\n",
              "      <td>11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3318</th>\n",
              "      <td>0.395</td>\n",
              "      <td>0.315</td>\n",
              "      <td>0.105</td>\n",
              "      <td>0.3515</td>\n",
              "      <td>0.1185</td>\n",
              "      <td>0.0910</td>\n",
              "      <td>0.1195</td>\n",
              "      <td>16</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3319</th>\n",
              "      <td>0.450</td>\n",
              "      <td>0.355</td>\n",
              "      <td>0.120</td>\n",
              "      <td>0.4120</td>\n",
              "      <td>0.1145</td>\n",
              "      <td>0.0665</td>\n",
              "      <td>0.1600</td>\n",
              "      <td>19</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-15e0d9ef-22c4-42da-8597-8aaa98742188')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-15e0d9ef-22c4-42da-8597-8aaa98742188 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-15e0d9ef-22c4-42da-8597-8aaa98742188');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-44fbcb8c-e13f-4a18-b574-f45eb517a499\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-44fbcb8c-e13f-4a18-b574-f45eb517a499')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-44fbcb8c-e13f-4a18-b574-f45eb517a499 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "text/plain": [
              "      Length  Diameter  Height  Whole weight  Shucked weight  Viscera weight  \\\n",
              "3315   0.605     0.475   0.180        0.9365          0.3940          0.2190   \n",
              "3316   0.700     0.525   0.190        1.6015          0.7070          0.3650   \n",
              "3317   0.530     0.420   0.130        0.8365          0.3745          0.1670   \n",
              "3318   0.395     0.315   0.105        0.3515          0.1185          0.0910   \n",
              "3319   0.450     0.355   0.120        0.4120          0.1145          0.0665   \n",
              "\n",
              "      Shell weight  Age  \n",
              "3315        0.2950   15  \n",
              "3316        0.4300   10  \n",
              "3317        0.2490   11  \n",
              "3318        0.1195   16  \n",
              "3319        0.1600   19  "
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "abalone_train = pd.read_csv(\n",
        "    \"https://storage.googleapis.com/download.tensorflow.org/data/abalone_train.csv\",\n",
        "    names=[\"Length\", \"Diameter\", \"Height\", \"Whole weight\", \"Shucked weight\",\n",
        "           \"Viscera weight\", \"Shell weight\", \"Age\"])\n",
        "\n",
        "abalone_train.tail()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hP22mdyPQ1_t"
      },
      "source": [
        "이 데이터세트에는 바다 고등류의 일종인 [전복](https://en.wikipedia.org/wiki/Abalone) 측정값 세트가 포함되어 있습니다.\n",
        "\n",
        "![an abalone shell](https://tensorflow.org/images/abalone_shell.jpg)\n",
        "\n",
        "[“전복 껍질”](https://www.flickr.com/photos/thenickster/16641048623/) ([Nicki Dugan Pogue](https://www.flickr.com/photos/thenickster/) 제공, CC BY-SA 2.0)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vlfGrk_9N-wf"
      },
      "source": [
        "이 데이터 세트의 명목상 작업은 다른 측정값으로부터 나이를 예측하는 것이므로 다음과 같이 훈련을 위해 특성과 레이블을 분리합니다.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "udOnDJOxNi7p"
      },
      "outputs": [],
      "source": [
        "abalone_features = abalone_train.copy()\n",
        "abalone_labels = abalone_features.pop('Age')\n",
        "\n",
        "# abalone_features = abalone_train.copy()\n",
        "# abalone_features = abalone_features.pop('Age')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "seK9n71-UBfT"
      },
      "source": [
        "이 데이터세트에서는 모든 특성을 동일하게 취급합니다. 다음과 같이 특성을 단일 NumPy 배열로 묶습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dp3N5McbUMwb",
        "outputId": "ffe74b6c-875b-4903-9a2d-4db84243ffe4"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[0.435, 0.335, 0.11 , ..., 0.136, 0.077, 0.097],\n",
              "       [0.585, 0.45 , 0.125, ..., 0.354, 0.207, 0.225],\n",
              "       [0.655, 0.51 , 0.16 , ..., 0.396, 0.282, 0.37 ],\n",
              "       ...,\n",
              "       [0.53 , 0.42 , 0.13 , ..., 0.374, 0.167, 0.249],\n",
              "       [0.395, 0.315, 0.105, ..., 0.118, 0.091, 0.119],\n",
              "       [0.45 , 0.355, 0.12 , ..., 0.115, 0.067, 0.16 ]])"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "abalone_features = np.array(abalone_features)\n",
        "abalone_features\n",
        "\n",
        "# abalone_features = np.array(abalone_features)\n",
        "# abalone_features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1C1yFOxLOdxh"
      },
      "source": [
        "다음으로, 회귀 모델로 나이를 예측합니다. 입력 텐서가 하나만 있으므로 여기에서는 `keras.Sequential` 모델이면 충분합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "d8zzNrZqOmfB"
      },
      "outputs": [],
      "source": [
        "abalone_model = tf.keras.Sequential([\n",
        "  layers.Dense(64),\n",
        "  layers.Dense(1)\n",
        "])\n",
        "\n",
        "abalone_model.compile(loss = tf.keras.losses.MeanSquaredError(),\n",
        "                      optimizer = tf.keras.optimizers.Adam())\n",
        "\n",
        "# abalone_model = tf.keras.Sequential([\n",
        "#     layers.Dense(64),\n",
        "#     layers.Dense(1)\n",
        "# ]\n",
        "# )\n",
        "\n",
        "# abalone_model.compile(loss = tf.keras.losses.MeanSquaredError(),\n",
        "#                       optimizer = tf.keras.optimizers.Adam())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j6IWeP78O2wE"
      },
      "source": [
        "해당 모델을 훈련하려면 특성과 레이블을 `Model.fit`로 전달합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uZdpCD92SN3Z",
        "outputId": "426467d7-98ac-427b-9838-7face295939c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 78.1784\n",
            "Epoch 2/10\n",
            "\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 13.3848\n",
            "Epoch 3/10\n",
            "\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 9.1198\n",
            "Epoch 4/10\n",
            "\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 8.1658\n",
            "Epoch 5/10\n",
            "\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 7.9116\n",
            "Epoch 6/10\n",
            "\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 7.9307\n",
            "Epoch 7/10\n",
            "\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 7.0697\n",
            "Epoch 8/10\n",
            "\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 7.2346\n",
            "Epoch 9/10\n",
            "\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 6.6091\n",
            "Epoch 10/10\n",
            "\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 6.6821\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7aa394401ae0>"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "abalone_model.fit(abalone_features, abalone_labels, epochs=10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GapLOj1OOTQH"
      },
      "source": [
        "지금까지 CSV 데이터를 사용하여 모델을 훈련하는 가장 기본적인 방법을 보았습니다. 다음으로 숫자 열을 정규화하기 위해 전처리를 적용하는 방법을 배웁니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B87Rd1SOUv02"
      },
      "source": [
        "## 기본 전처리"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yCrB2Jd-U0Vt"
      },
      "source": [
        "모델에 대한 입력을 정규화하면 좋습니다. Keras 전처리 레이어는 이 정규화를 모델에 빌드하는 편리한 방법을 제공합니다.\n",
        "\n",
        "`tf.keras.layers.Normalization` 레이어는 각 열의 평균과 분산을 미리 계산하고 이를 사용하여 데이터를 정규화합니다.\n",
        "\n",
        "먼저 레이어를 만듭니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "H2WQpDU5VRk7"
      },
      "outputs": [],
      "source": [
        "normalize = layers.Normalization()\n",
        "\n",
        "# normalize = layers.Normalization()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hGgEZE-7Vpt6"
      },
      "source": [
        "그런 다음 `Normalization.adapt` 메서드를 사용하여 정규화 레이어를 데이터에 맞게 조정합니다.\n",
        "\n",
        "참고: `PreprocessingLayer.adapt` 메서드와 함께 훈련 데이터만 사용하고 검증 또는 테스트 데이터는 사용하지 마세요."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "2WgOPIiOVpLg"
      },
      "outputs": [],
      "source": [
        "normalize.adapt(abalone_features)\n",
        "\n",
        "# normalize.adapt(abalone_features)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rE6vh0byV7cE"
      },
      "source": [
        "그런 다음 모델에서 정규화 레이어를 사용합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "quPcZ9dTWA9A",
        "outputId": "39024d9e-daed-4abf-95e6-e704b55cf28e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 99.5707 \n",
            "Epoch 2/10\n",
            "\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 66.5461\n",
            "Epoch 3/10\n",
            "\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 23.9384\n",
            "Epoch 4/10\n",
            "\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 5.8917\n",
            "Epoch 5/10\n",
            "\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.7552\n",
            "Epoch 6/10\n",
            "\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.9065\n",
            "Epoch 7/10\n",
            "\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.8834\n",
            "Epoch 8/10\n",
            "\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.9113\n",
            "Epoch 9/10\n",
            "\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.6888\n",
            "Epoch 10/10\n",
            "\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 5.0762\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7aa390a3f1c0>"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "norm_abalone_model = tf.keras.Sequential([\n",
        "  normalize,\n",
        "  layers.Dense(64),\n",
        "  layers.Dense(1)\n",
        "])\n",
        "\n",
        "norm_abalone_model.compile(loss = tf.keras.losses.MeanSquaredError(),\n",
        "                           optimizer = tf.keras.optimizers.Adam())\n",
        "\n",
        "norm_abalone_model.fit(abalone_features, abalone_labels, epochs=10)\n",
        "\n",
        "\n",
        "# norm_abalone_model = tf.keras.Sequential([\n",
        "#     normalize,\n",
        "#     layers.Dense(64),\n",
        "#     layers.Dense(1)\n",
        "# ])\n",
        "\n",
        "# norm_abalone_model.compile(loss = tf.keras.losses.MeanSquaredError(),\n",
        "#                            optimizer = tf.keras.optimizers.Adam())\n",
        "\n",
        "# norm_abalone_model.fit(abalone_features, abalone_labels, epochs=10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wuqj601Qw0Ml"
      },
      "source": [
        "## 혼합 데이터 유형\n",
        "\n",
        "The \"Titanic\" dataset contains information about the passengers on the Titanic. The nominal task on this dataset is to predict who survived.\n",
        "\n",
        "![The Titanic](https://github.com/tensorflow/docs-l10n/blob/master/site/ko/tutorials/load_data/images/csv/Titanic.jpg?raw=1)\n",
        "\n",
        "Image [from Wikimedia](https://commons.wikimedia.org/wiki/File:RMS_Titanic_3.jpg)\n",
        "\n",
        "The raw data can easily be loaded as a Pandas `DataFrame`, but is not immediately usable as input to a TensorFlow model.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "GS-dBMpuYMnz",
        "outputId": "27eaeccb-97a1-4639-8752-fbb974d212f7"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "summary": "{\n  \"name\": \"titanic\",\n  \"rows\": 627,\n  \"fields\": [\n    {\n      \"column\": \"survived\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          1,\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"sex\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"female\",\n          \"male\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"age\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 12.511817629565815,\n        \"min\": 0.75,\n        \"max\": 80.0,\n        \"num_unique_values\": 76,\n        \"samples\": [\n          28.0,\n          59.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"n_siblings_spouses\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1,\n        \"min\": 0,\n        \"max\": 8,\n        \"num_unique_values\": 7,\n        \"samples\": [\n          1,\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"parch\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 5,\n        \"num_unique_values\": 6,\n        \"samples\": [\n          0,\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"fare\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 54.5977304994563,\n        \"min\": 0.0,\n        \"max\": 512.3292,\n        \"num_unique_values\": 216,\n        \"samples\": [\n          25.9292,\n          5.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"class\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          \"Third\",\n          \"First\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"deck\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 8,\n        \"samples\": [\n          \"C\",\n          \"D\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"embark_town\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 4,\n        \"samples\": [\n          \"Cherbourg\",\n          \"unknown\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"alone\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"y\",\n          \"n\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}",
              "type": "dataframe",
              "variable_name": "titanic"
            },
            "text/html": [
              "\n",
              "  <div id=\"df-d1182866-75ed-4325-8991-b34d16e1879f\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>survived</th>\n",
              "      <th>sex</th>\n",
              "      <th>age</th>\n",
              "      <th>n_siblings_spouses</th>\n",
              "      <th>parch</th>\n",
              "      <th>fare</th>\n",
              "      <th>class</th>\n",
              "      <th>deck</th>\n",
              "      <th>embark_town</th>\n",
              "      <th>alone</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>male</td>\n",
              "      <td>22.0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>7.2500</td>\n",
              "      <td>Third</td>\n",
              "      <td>unknown</td>\n",
              "      <td>Southampton</td>\n",
              "      <td>n</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>female</td>\n",
              "      <td>38.0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>71.2833</td>\n",
              "      <td>First</td>\n",
              "      <td>C</td>\n",
              "      <td>Cherbourg</td>\n",
              "      <td>n</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>female</td>\n",
              "      <td>26.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>7.9250</td>\n",
              "      <td>Third</td>\n",
              "      <td>unknown</td>\n",
              "      <td>Southampton</td>\n",
              "      <td>y</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>female</td>\n",
              "      <td>35.0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>53.1000</td>\n",
              "      <td>First</td>\n",
              "      <td>C</td>\n",
              "      <td>Southampton</td>\n",
              "      <td>n</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>male</td>\n",
              "      <td>28.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>8.4583</td>\n",
              "      <td>Third</td>\n",
              "      <td>unknown</td>\n",
              "      <td>Queenstown</td>\n",
              "      <td>y</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-d1182866-75ed-4325-8991-b34d16e1879f')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-d1182866-75ed-4325-8991-b34d16e1879f button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-d1182866-75ed-4325-8991-b34d16e1879f');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-9ed14521-6ed4-4fd4-8cc8-770bfdcf22a4\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-9ed14521-6ed4-4fd4-8cc8-770bfdcf22a4')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-9ed14521-6ed4-4fd4-8cc8-770bfdcf22a4 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "text/plain": [
              "   survived     sex   age  n_siblings_spouses  parch     fare  class     deck  \\\n",
              "0         0    male  22.0                   1      0   7.2500  Third  unknown   \n",
              "1         1  female  38.0                   1      0  71.2833  First        C   \n",
              "2         1  female  26.0                   0      0   7.9250  Third  unknown   \n",
              "3         1  female  35.0                   1      0  53.1000  First        C   \n",
              "4         0    male  28.0                   0      0   8.4583  Third  unknown   \n",
              "\n",
              "   embark_town alone  \n",
              "0  Southampton     n  \n",
              "1    Cherbourg     n  \n",
              "2  Southampton     y  \n",
              "3  Southampton     n  \n",
              "4   Queenstown     y  "
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "titanic = pd.read_csv(\"https://storage.googleapis.com/tf-datasets/titanic/train.csv\")\n",
        "titanic.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "D8rCGIK1ZzKx"
      },
      "outputs": [],
      "source": [
        "titanic_features = titanic.copy()\n",
        "titanic_labels = titanic_features.pop('survived')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "urHOwpCDYtcI"
      },
      "source": [
        "데이터 유형과 범위가 다르기 때문에 단순히 특성을 NumPy 배열에 쌓아서 `keras.Sequential` 모델로 전달할 수 없습니다. 각 열을 개별적으로 처리해야 합니다.\n",
        "\n",
        "한 가지 옵션으로 데이터를 오프라인으로 전처리(원하는 도구 사용)하여 범주형 열을 숫자 열로 변환한 다음 처리된 출력을 TensorFlow 모델에 전달할 수 있습니다. 이 접근 방식의 단점은 모델을 저장하고 내보내는 경우 전처리가 함께 저장되지 않는다는 것입니다. Keras 전처리 레이어는 모델의 일부이기 때문에 이 문제를 피할 수 있습니다.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bta4Sx0Zau5v"
      },
      "source": [
        "이 예제에서는 [Keras 함수형 API](https://www.tensorflow.org/guide/keras/functional)를 사용하여 전처리 로직을 구현하는 모델을 빌드합니다. [하위 클래스화](https://www.tensorflow.org/guide/keras/custom_layers_and_models)하여 같은 작업을 수행할 수도 있습니다.\n",
        "\n",
        "함수형 API는 \"기호화된\" 텐서에서 작동합니다. 정상적인 \"즉시(eager)\" 텐서에는 값이 있습니다. 대조적으로 이러한 \"기호화된\" 텐서에는 값이 없습니다. 대신에 실행되는 작업을 추적하고 나중에 실행할 수 있는 계산 표현을 작성합니다. 다음은 간단한 예제입니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "730F16_97D-3",
        "outputId": "aa275926-f646-40ef-f3dc-be45b058ff26"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<KerasTensor shape=(None,), dtype=float32, sparse=False, name=keras_tensor_9>"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Create a symbolic input\n",
        "input = tf.keras.Input(shape=(), dtype=tf.float32)\n",
        "\n",
        "# Perform a calculation using the input\n",
        "result = 2*input + 1\n",
        "\n",
        "# the result doesn't have a value\n",
        "result\n",
        "\n",
        "\n",
        "# input = tf.keras.Input(shape=(), dtype=tf.float32)\n",
        "# result = 2*input + 1\n",
        "# result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "RtcNXWB18kMJ"
      },
      "outputs": [],
      "source": [
        "calc = tf.keras.Model(inputs=input, outputs=result)\n",
        "\n",
        "# calc = tf.keras.Model(inputs=input, outputs=result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 304
        },
        "id": "fUGQOUqZ8sa-",
        "outputId": "d82f4dc0-5631-417c-de93-f83b649513a8"
      },
      "outputs": [
        {
          "ename": "ValueError",
          "evalue": "Inputs to a layer should be tensors. Got '1' (of type <class 'int'>) as input for layer 'functional_2'.",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-98491f4f551a>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcalc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcalc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    120\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m             \u001b[0;31m# `keras.config.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    123\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/layers/input_spec.py\u001b[0m in \u001b[0;36massert_input_compatibility\u001b[0;34m(input_spec, inputs, layer_name)\u001b[0m\n\u001b[1;32m    174\u001b[0m         \u001b[0;31m# which does not have a `shape` attribute.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"shape\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 176\u001b[0;31m             raise ValueError(\n\u001b[0m\u001b[1;32m    177\u001b[0m                 \u001b[0;34mf\"Inputs to a layer should be tensors. Got '{x}' \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m                 \u001b[0;34mf\"(of type {type(x)}) as input for layer '{layer_name}'.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Inputs to a layer should be tensors. Got '1' (of type <class 'int'>) as input for layer 'functional_2'."
          ]
        }
      ],
      "source": [
        "print(calc(1).numpy())\n",
        "print(calc(2).numpy())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rNS9lT7f6_U2"
      },
      "source": [
        "전처리 모델을 빌드하려면 먼저 CSV 열의 이름 및 데이터 유형과 일치하는 기호화된 `tf.keras.Input` 객체 세트를 빌드합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5WODe_1da3yw"
      },
      "outputs": [],
      "source": [
        "inputs = {}\n",
        "\n",
        "for name, column in titanic_features.items():\n",
        "  dtype = column.dtype\n",
        "  if dtype == object:\n",
        "    dtype = tf.string\n",
        "  else:\n",
        "    dtype = tf.float32\n",
        "\n",
        "  inputs[name] = tf.keras.Input(shape=(1,), name=name, dtype=dtype)\n",
        "\n",
        "inputs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aaheJFmymq8l"
      },
      "source": [
        "전처리 논리의 첫 번째 단계는 숫자 입력을 함께 연결하고 이를 정규화 레이어를 통해 실행하는 것입니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wPRC_E6rkp8D"
      },
      "outputs": [],
      "source": [
        "numeric_inputs = {name:input for name,input in inputs.items()\n",
        "                  if input.dtype==tf.float32}\n",
        "\n",
        "x = layers.Concatenate()(list(numeric_inputs.values()))\n",
        "norm = layers.Normalization()\n",
        "norm.adapt(np.array(titanic[numeric_inputs.keys()]))\n",
        "all_numeric_inputs = norm(x)\n",
        "\n",
        "all_numeric_inputs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-JoR45Uj712l"
      },
      "source": [
        "나중에 연결할 수 있도록 모든 기호화된 전처리 결과를 수집합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M7jIJw5XntdN"
      },
      "outputs": [],
      "source": [
        "preprocessed_inputs = [all_numeric_inputs]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r0Hryylyosfm"
      },
      "source": [
        "문자열 입력의 경우 `tf.keras.layers.StringLookup` 함수를 사용하여 문자열로부터 어휘의 정수 인덱스로 매핑합니다. 그런 다음 `tf.keras.layers.CategoryEncoding`을 사용하여 인덱스를 모델에 적합한 `float32` 데이터로 변환합니다.\n",
        "\n",
        "`tf.keras.layers.CategoryEncoding` 레이어의 기본 설정은 각 입력에 대해 원-핫 벡터를 생성하는 것입니다. `tf.keras.layers.Embedding`도 작동합니다. 이 주제에 대한 자세한 내용은 [전처리 레이어 작업](https://www.tensorflow.org/guide/keras/preprocessing_layers) 가이드 및 [Keras 전처리 레이어를 사용하여 구조화된 데이터 분류](../structured_data/preprocessing_layers.ipynb) 튜토리얼을 확인하세요."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "79fi1Cgan2YV"
      },
      "outputs": [],
      "source": [
        "for name, input in inputs.items():\n",
        "  if input.dtype == tf.float32:\n",
        "    continue\n",
        "\n",
        "  lookup = layers.StringLookup(vocabulary=np.unique(titanic_features[name]))\n",
        "  one_hot = layers.CategoryEncoding(num_tokens=lookup.vocabulary_size())\n",
        "\n",
        "  x = lookup(input)\n",
        "  x = one_hot(x)\n",
        "  preprocessed_inputs.append(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wnhv0T7itnc7"
      },
      "source": [
        "`inputs` 및 `preprocessed_inputs` 모음을 사용하여 전처리된 모든 입력을 함께 연결하고 전처리를 처리하는 모델을 빌드할 수 있습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XJRzUTe8ukXc"
      },
      "outputs": [],
      "source": [
        "preprocessed_inputs_cat = layers.Concatenate()(preprocessed_inputs)\n",
        "\n",
        "titanic_preprocessing = tf.keras.Model(inputs, preprocessed_inputs_cat)\n",
        "\n",
        "tf.keras.utils.plot_model(model = titanic_preprocessing , rankdir=\"LR\", dpi=72, show_shapes=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PNHxrNW8vdda"
      },
      "source": [
        "이 모델은 입력 전처리만 포함합니다. 이를 실행하여 데이터에 어떤 영향을 미치는지 확인할 수 있습니다. Keras 모델은 Pandas <code>DataFrames</code>를 자동으로 변환하지 않습니다. 왜냐하면 하나의 텐서로 변환해야 하는지 아니면 텐서 사전으로 변환해야 하는지가 명확하지 않기 때문입니다. 따라서 이를 텐서 사전으로 변환합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5YjdYyMEacwQ"
      },
      "outputs": [],
      "source": [
        "titanic_features_dict = {name: np.array(value)\n",
        "                         for name, value in titanic_features.items()}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0nKJYoPByada"
      },
      "source": [
        "첫 번째 훈련 예제를 잘라서 이 전처리 모델로 전달하면 숫자 특성과 문자열 원-핫이 모두 함께 연결된 것을 볼 수 있습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SjnmU8PSv8T3"
      },
      "outputs": [],
      "source": [
        "features_dict = {name:values[:1] for name, values in titanic_features_dict.items()}\n",
        "titanic_preprocessing(features_dict)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qkBf4LvmzMDp"
      },
      "source": [
        "이제 이 위에 모델을 빌드합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "coIPtGaCzUV7"
      },
      "outputs": [],
      "source": [
        "def titanic_model(preprocessing_head, inputs):\n",
        "  body = tf.keras.Sequential([\n",
        "    layers.Dense(64),\n",
        "    layers.Dense(1)\n",
        "  ])\n",
        "\n",
        "  preprocessed_inputs = preprocessing_head(inputs)\n",
        "  result = body(preprocessed_inputs)\n",
        "  model = tf.keras.Model(inputs, result)\n",
        "\n",
        "  model.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
        "                optimizer=tf.keras.optimizers.Adam())\n",
        "  return model\n",
        "\n",
        "titanic_model = titanic_model(titanic_preprocessing, inputs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LK5uBQQF2KbZ"
      },
      "source": [
        "모델을 훈련할 때 특성 사전을 `x`로, 레이블을 `y`로 전달합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D1gVfwJ61ejz"
      },
      "outputs": [],
      "source": [
        "titanic_model.fit(x=titanic_features_dict, y=titanic_labels, epochs=10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LxgJarZk3bfH"
      },
      "source": [
        "전처리는 모델의 일부이므로 모델을 저장하고 다른 곳에 다시 로드하여도 동일한 결과를 얻을 수 있습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ay-8ymNA2ZCh"
      },
      "outputs": [],
      "source": [
        "titanic_model.save('test')\n",
        "reloaded = tf.keras.models.load_model('test')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qm6jMTpD20lK"
      },
      "outputs": [],
      "source": [
        "features_dict = {name:values[:1] for name, values in titanic_features_dict.items()}\n",
        "\n",
        "before = titanic_model(features_dict)\n",
        "after = reloaded(features_dict)\n",
        "assert (before-after)<1e-3\n",
        "print(before)\n",
        "print(after)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7VsPlxIRZpXf"
      },
      "source": [
        "## tf.data 사용하기\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NyVDCwGzR5HW"
      },
      "source": [
        "이전 섹션에서는 모델을 훈련하는 동안 모델의 내장 데이터 셔플링 및 배치에 의존했습니다.\n",
        "\n",
        "입력 데이터 파이프라인을 더 많이 제어해야 하거나 메모리에 쉽게 맞출 수 없는 데이터를 사용해야 하는 경우 `tf.data`를 사용합니다.\n",
        "\n",
        "더 많은 예를 보려면 [`tf.data`: TensorFlow 입력 파이프라인 빌드](../../guide/data.ipynb) 가이드를 참조하세요."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gP5Y1jM2Sor0"
      },
      "source": [
        "### 인메모리 데이터에서\n",
        "\n",
        "CSV 데이터에 `tf.data`를 적용하는 첫 번째 예제로 다음 코드를 고려할 경우 이전 섹션의 특성 사전을 수동으로 분할합니다. 각 인덱스에는 각 특성에 대해 이러한 인덱스를 사용합니다.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i8wE-MVuVu7_"
      },
      "outputs": [],
      "source": [
        "import itertools\n",
        "\n",
        "def slices(features):\n",
        "  for i in itertools.count():\n",
        "    # For each feature take index `i`\n",
        "    example = {name:values[i] for name, values in features.items()}\n",
        "    yield example"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cQ3RTbS9YEal"
      },
      "source": [
        "이것을 실행하고 첫 번째 예제를 출력합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wwq8XK88WwFk"
      },
      "outputs": [],
      "source": [
        "for example in slices(titanic_features_dict):\n",
        "  for name, value in example.items():\n",
        "    print(f\"{name:19s}: {value}\")\n",
        "  break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vvp8Dct6YOIE"
      },
      "source": [
        "메모리 데이터 로더에서 가장 기본적인 `tf.data.Dataset`은 `Dataset.from_tensor_slices` 생성자입니다. 이것은 TensorFlow에서 위의 `slices` 함수의 일반화된 버전을 구현하는 `tf.data.Dataset`를 반환합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2gEJthslYxeV"
      },
      "outputs": [],
      "source": [
        "features_ds = tf.data.Dataset.from_tensor_slices(titanic_features_dict)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ZC0rTpMZMZK"
      },
      "source": [
        "다른 Python iterable과 마찬가지로 `tf.data.Dataset`에 대해 반복할 수 있습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gOHbiefaY4ag"
      },
      "outputs": [],
      "source": [
        "for example in features_ds:\n",
        "  for name, value in example.items():\n",
        "    print(f\"{name:19s}: {value}\")\n",
        "  break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uwcFoVJWZY5F"
      },
      "source": [
        "`from_tensor_slices` 함수는 모든 구조의 중첩된 사전 또는 튜플을 처리할 수 있습니다. 다음 코드는 `(features_dict, labels)` 쌍의 데이터세트를 만듭니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xIHGBy76Zcrx"
      },
      "outputs": [],
      "source": [
        "titanic_ds = tf.data.Dataset.from_tensor_slices((titanic_features_dict, titanic_labels))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gQwxitt8c2GK"
      },
      "source": [
        "이 `Dataset`를 사용하여 모델을 훈련하려면 최소한 데이터를 `shuffle`하고 `batch` 처리해야 합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SbJcbldhddeC"
      },
      "outputs": [],
      "source": [
        "titanic_batches = titanic_ds.shuffle(len(titanic_labels)).batch(32)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-4FRqhRFuoJx"
      },
      "source": [
        "`features` 및 `labels`를 `Model.fit`에 전달하는 대신 데이터세트를 전달합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8yXkNPumdBtB"
      },
      "outputs": [],
      "source": [
        "titanic_model.fit(titanic_batches, epochs=5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qXuibiv9exT7"
      },
      "source": [
        "### 단일 파일로부터\n",
        "\n",
        "지금까지 이 튜토리얼은 인메모리 데이터로 작업했습니다. `tf.data`는 데이터 파이프라인을 빌드하기 위한 확장성이 뛰어난 툴킷이며 CSV 파일 로드를 처리하는 몇 가지 기능을 제공합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ncf5t6tgL5ZI"
      },
      "outputs": [],
      "source": [
        "titanic_file_path = tf.keras.utils.get_file(\"train.csv\", \"https://storage.googleapis.com/tf-datasets/titanic/train.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t4N-plO4tDXd"
      },
      "source": [
        "이제 파일에서 CSV 데이터를 읽고 `tf.data.Dataset`를 작성합니다.\n",
        "\n",
        "(전체 설명서는 `tf.data.experimental.make_csv_dataset`를 참조하세요.)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yIbUscB9sqha"
      },
      "outputs": [],
      "source": [
        "titanic_csv_ds = tf.data.experimental.make_csv_dataset(\n",
        "    titanic_file_path,\n",
        "    batch_size=5, # Artificially small to make examples easier to show.\n",
        "    label_name='survived',\n",
        "    num_epochs=1,\n",
        "    ignore_errors=True,)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sf3v3BKgy4AG"
      },
      "source": [
        "이 함수에는 많은 편리한 특성이 포함되어 있어 데이터 작업이 용이합니다. 여기에는 다음이 포함되어 있습니다.\n",
        "\n",
        "- 열 헤더를 사전 키로 사용.\n",
        "- 각 열의 유형을 자동으로 결정.\n",
        "\n",
        "주의: `tf.data.experimental.make_csv_dataset`에서 `num_epochs` 인수를 설정해야 합니다. 그렇지 않으면 `tf.data.Dataset`의 기본 동작은 루프를 무한히 반복하는 것입니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v4oMO9MIxgTG"
      },
      "outputs": [],
      "source": [
        "for batch, label in titanic_csv_ds.take(1):\n",
        "  for key, value in batch.items():\n",
        "    print(f\"{key:20s}: {value}\")\n",
        "  print()\n",
        "  print(f\"{'label':20s}: {label}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k-TgA6o2Ja6U"
      },
      "source": [
        "참고: 위의 셀을 두 번 실행하면 다른 결과가 생성됩니다. `tf.data.experimental.make_csv_dataset`의 기본 설정에는 `shuffle_buffer_size=1000`이 포함되며, 이는 이 작은 데이터세트에는 충분하지만 실제 데이터세트에는 그렇지 않을 수 있습니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d6uviU_KCCWD"
      },
      "source": [
        "즉시 데이터 압축을 풀 수도 있습니다. 다음은 [대도시 주간 교통량 데이터세트](https://archive.ics.uci.edu/ml/datasets/Metro+Interstate+Traffic+Volume)가 포함된 gzip으로 압축된 CSV 파일입니다.\n",
        "\n",
        "![교통 정체.](https://github.com/tensorflow/docs-l10n/blob/master/site/ko/tutorials/load_data/images/csv/traffic.jpg?raw=1)\n",
        "\n",
        "이미지 출처: [Wikimedia](https://commons.wikimedia.org/wiki/File:Trafficjam.jpg)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kT7oZI2E46Q8"
      },
      "outputs": [],
      "source": [
        "traffic_volume_csv_gz = tf.keras.utils.get_file(\n",
        "    'Metro_Interstate_Traffic_Volume.csv.gz',\n",
        "    \"https://archive.ics.uci.edu/ml/machine-learning-databases/00492/Metro_Interstate_Traffic_Volume.csv.gz\",\n",
        "    cache_dir='.', cache_subdir='traffic')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F-IOsFHbCw0i"
      },
      "source": [
        "압축된 파일로부터 직접 읽도록 `compression_type` 인수를 설정합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ar0MPEVJ5NeA"
      },
      "outputs": [],
      "source": [
        "traffic_volume_csv_gz_ds = tf.data.experimental.make_csv_dataset(\n",
        "    traffic_volume_csv_gz,\n",
        "    batch_size=256,\n",
        "    label_name='traffic_volume',\n",
        "    num_epochs=1,\n",
        "    compression_type=\"GZIP\")\n",
        "\n",
        "for batch, label in traffic_volume_csv_gz_ds.take(1):\n",
        "  for key, value in batch.items():\n",
        "    print(f\"{key:20s}: {value[:5]}\")\n",
        "  print()\n",
        "  print(f\"{'label':20s}: {label[:5]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p12Y6tGq8D6M"
      },
      "source": [
        "참고: `tf.data` 파이프라인에서 해당 날짜-시간 문자열을 파싱해야 하는 경우 `tfa.text.parse_time`을 사용할 수 있습니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EtrAXzYGP3l0"
      },
      "source": [
        "### 캐싱"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fN2dL_LRP83r"
      },
      "source": [
        "CSV 데이터를 파싱하는 데 약간의 오버헤드가 있습니다. 작은 크기의 모델의 경우 이때 훈련 병목 현상이 발생할 수 있습니다.\n",
        "\n",
        "사용 사례에 따라 CSV 데이터가 첫 epoch에서만 구문 분석되도록 `Dataset.cache` 또는 `tf.data.Dataset.snapshot`을 사용하는 것이 좋습니다.\n",
        "\n",
        "`cache`와 `snapshot` 메서드의 주요 차이점은 `cache` 파일은 이를 생성한 TensorFlow 프로세스에서만 사용할 수 있다는 것입니다. 다만, `snapshot` 파일은 다른 프로세스에서 읽을 수 있습니다.\n",
        "\n",
        "예를 들어, `traffic_volume_csv_gz_ds`를 20번 반복하는 데 캐싱 없이는 약 15초, 캐싱이 있으면 약 2초가 걸릴 수 있습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qk38Sw4MO4eh"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "for i, (batch, label) in enumerate(traffic_volume_csv_gz_ds.repeat(20)):\n",
        "  if i % 40 == 0:\n",
        "    print('.', end='')\n",
        "print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pN3HtDONh5TX"
      },
      "source": [
        "참고: `Dataset.cache`는 첫 번째 epoch의 데이터를 저장하고 순서대로 재생합니다. 따라서 `cache` 메서드를 사용하면 파이프라인의 초기에 모든 셔플이 비활성화됩니다. 아래에서 `Dataset.shuffle`은 `Dataset.cache` 뒤에 다시 추가됩니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r5Jj72MrPbnh"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "caching = traffic_volume_csv_gz_ds.cache().shuffle(1000)\n",
        "\n",
        "for i, (batch, label) in enumerate(caching.shuffle(1000).repeat(20)):\n",
        "  if i % 40 == 0:\n",
        "    print('.', end='')\n",
        "print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wN7uUBjmgNZ9"
      },
      "source": [
        "참고: `tf.data.Dataset.snapshot` 파일은 사용 중인 데이터세트를 *임시* 저장할 경우 사용합니다. 이것은 장기 저장을 위한 형식이 *아닙니다*. 파일 형식은 내부 세부 정보로 간주되며 TensorFlow 버전 간의 호환은 보장되지 않습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PHGD1E8ktUvW"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "snapshotting = traffic_volume_csv_gz_ds.snapshot('titanic.tfsnap').shuffle(1000)\n",
        "\n",
        "for i, (batch, label) in enumerate(snapshotting.shuffle(1000).repeat(20)):\n",
        "  if i % 40 == 0:\n",
        "    print('.', end='')\n",
        "print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fUSSegnMCGRz"
      },
      "source": [
        "CSV 파일 로드로 인해 데이터 로드가 느려지고 `Dataset.cache` 및 `tf.data.Dataset.snapshot`이 사용 사례에 충분하지 않은 경우, 데이터를 보다 간소화된 형식으로 다시 인코딩하는 것이 좋습니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M0iGXv9pC5kr"
      },
      "source": [
        "### 여러 파일"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9FFzHQrCDH4w"
      },
      "source": [
        "지금까지 이 섹션의 모든 예제는 `tf.data` 없이 쉽게 수행할 수 있었습니다. `tf.data`를 사용하여 실제로 작업을 단순화할 수 있는 한 예는 파일 모음을 처리할 경우입니다.\n",
        "\n",
        "예를 들어 [문자 글꼴 이미지](https://archive.ics.uci.edu/ml/datasets/Character+Font+Images) 데이터세트는 글꼴당 하나씩, csv 파일 모음으로 배포됩니다.\n",
        "\n",
        "![글꼴](https://github.com/tensorflow/docs-l10n/blob/master/site/ko/tutorials/load_data/images/csv/fonts.jpg?raw=1)\n",
        "\n",
        "<a href=\"https://pixabay.com/users/wilhei-883152/?utm_source=link-attribution&amp;utm_medium=referral&amp;utm_campaign=image&amp;utm_content=705667\">Pixabay</a>에서 <a href=\"https://pixabay.com/?utm_source=link-attribution&amp;utm_medium=referral&amp;utm_campaign=image&amp;utm_content=705667\">Willi Heidelbach</a>가 제공한 이미지\n",
        "\n",
        "데이터세트를 다운로드하고 내부 파일을 검토합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RmVknMdJh5ks"
      },
      "outputs": [],
      "source": [
        "fonts_zip = tf.keras.utils.get_file(\n",
        "    'fonts.zip',  \"https://archive.ics.uci.edu/ml/machine-learning-databases/00417/fonts.zip\",\n",
        "    cache_dir='.', cache_subdir='fonts',\n",
        "    extract=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xsDlMCnyi55e"
      },
      "outputs": [],
      "source": [
        "import pathlib\n",
        "font_csvs =  sorted(str(p) for p in pathlib.Path('fonts').glob(\"*.csv\"))\n",
        "\n",
        "font_csvs[:10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lRAEJx9ROAGl"
      },
      "outputs": [],
      "source": [
        "len(font_csvs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "19Udrw9iG-FS"
      },
      "source": [
        "많은 파일을 처리할 경우 glob 스타일의 `file_pattern`을 `tf.data.experimental.make_csv_dataset` 함수에 전달할 수 있습니다. 파일의 순서는 각 반복마다 뒤섞입니다.\n",
        "\n",
        "`num_parallel_reads` 인수를 사용하여 병렬로 읽고 함께 인터리브 처리되는 파일의 수를 설정합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6TSUNdT6iG58"
      },
      "outputs": [],
      "source": [
        "fonts_ds = tf.data.experimental.make_csv_dataset(\n",
        "    file_pattern = \"fonts/*.csv\",\n",
        "    batch_size=10, num_epochs=1,\n",
        "    num_parallel_reads=20,\n",
        "    shuffle_buffer_size=10000)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XMoexinLHYFa"
      },
      "source": [
        "이러한 CSV 파일의 이미지는 단일 행으로 평면화되어 있습니다. 열 이름의 형식은 `r{row}c{column}`입니다. 다음은 첫 번째 배치입니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RmFvBWxxi3pq"
      },
      "outputs": [],
      "source": [
        "for features in fonts_ds.take(1):\n",
        "  for i, (name, value) in enumerate(features.items()):\n",
        "    if i>15:\n",
        "      break\n",
        "    print(f\"{name:20s}: {value}\")\n",
        "print('...')\n",
        "print(f\"[total: {len(features)} features]\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xrC3sKdeOhb5"
      },
      "source": [
        "#### 선택 사항: 패킹 필드\n",
        "\n",
        "여러분은 아마도 이와 같이 별도의 열에 있는 각 픽셀로 작업하고 싶지는 않을 것입니다. 이 데이터세트를 사용하기 전에 픽셀을 이미지 텐서로 패킹해야 합니다.\n",
        "\n",
        "다음은 각 예제의 이미지를 빌드하기 위해 열 이름을 파싱하는 코드입니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hct5EMEWNyfH"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "\n",
        "def make_images(features):\n",
        "  image = [None]*400\n",
        "  new_feats = {}\n",
        "\n",
        "  for name, value in features.items():\n",
        "    match = re.match('r(\\d+)c(\\d+)', name)\n",
        "    if match:\n",
        "      image[int(match.group(1))*20+int(match.group(2))] = value\n",
        "    else:\n",
        "      new_feats[name] = value\n",
        "\n",
        "  image = tf.stack(image, axis=0)\n",
        "  image = tf.reshape(image, [20, 20, -1])\n",
        "  new_feats['image'] = image\n",
        "\n",
        "  return new_feats"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "61qy8utAwARP"
      },
      "source": [
        "데이터세트의 각 배치에 해당 함수를 적용합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DJnnfIW9baE4"
      },
      "outputs": [],
      "source": [
        "fonts_image_ds = fonts_ds.map(make_images)\n",
        "\n",
        "for features in fonts_image_ds.take(1):\n",
        "  break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ThqrthGwHSm"
      },
      "source": [
        "결과 이미지를 플롯합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I5dcey31T_tk"
      },
      "outputs": [],
      "source": [
        "from matplotlib import pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(6,6), dpi=120)\n",
        "\n",
        "for n in range(9):\n",
        "  plt.subplot(3,3,n+1)\n",
        "  plt.imshow(features['image'][..., n])\n",
        "  plt.title(chr(features['m_label'][n]))\n",
        "  plt.axis('off')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7-nNR0Nncdd1"
      },
      "source": [
        "## 하위 수준 함수"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3jiGZeUijJNd"
      },
      "source": [
        "지금까지 이 튜토리얼은 csv 데이터를 읽기 위한 가장 높은 수준의 유틸리티에 중점을 두었습니다. 사용 사례가 이 기본 패턴에 맞지 않는 경우 고급 사용자에게 도움이 될 수 있는 다른 두 가지 API가 있습니다.\n",
        "\n",
        "- `tf.io.decode_csv`: 텍스트 줄을 CSV 열 텐서 목록으로 파싱하는 함수입니다.\n",
        "- `tf.data.experimental.CsvDataset`: 하위 수준 CSV 데이터세트 생성자입니다.\n",
        "\n",
        "이 섹션에서는 `tf.data.experimental.make_csv_dataset`에서 제공하는 기능을 다시 만들어 이 하위 수준 기능을 사용하는 방법을 보여줍니다.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LL_ixywomOHW"
      },
      "source": [
        "### `tf.io.decode_csv`\n",
        "\n",
        "이 함수는 문자열 또는 문자열 목록을 열 목록으로 디코딩합니다.\n",
        "\n",
        "`tf.data.experimental.make_csv_dataset`과 달리 이 함수는 열 데이터 유형을 추측하지 않습니다. 각 열에 대해 올바른 유형의 값이 포함된 `record_defaults` 목록을 제공하여 열 유형을 지정합니다.\n",
        "\n",
        "`tf.io.decode_csv`를 사용하여 타이타닉 데이터를 **문자열로** 읽기 위해 다음과 같이 할 수 있습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m1D2C-qdlqeW"
      },
      "outputs": [],
      "source": [
        "text = pathlib.Path(titanic_file_path).read_text()\n",
        "lines = text.split('\\n')[1:-1]\n",
        "\n",
        "all_strings = [str()]*10\n",
        "all_strings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9W4UeJYyHPx5"
      },
      "outputs": [],
      "source": [
        "features = tf.io.decode_csv(lines, record_defaults=all_strings)\n",
        "\n",
        "for f in features:\n",
        "  print(f\"type: {f.dtype.name}, shape: {f.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j8TaHSQFoQL4"
      },
      "source": [
        "실제 유형으로 파싱하려면 해당 유형의 `record_defaults` 목록을 만듭니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rzUjR59yoUe1"
      },
      "outputs": [],
      "source": [
        "print(lines[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7sPTunxwoeWU"
      },
      "outputs": [],
      "source": [
        "titanic_types = [int(), str(), float(), int(), int(), float(), str(), str(), str(), str()]\n",
        "titanic_types"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n3NlViCzoB7F"
      },
      "outputs": [],
      "source": [
        "features = tf.io.decode_csv(lines, record_defaults=titanic_types)\n",
        "\n",
        "for f in features:\n",
        "  print(f\"type: {f.dtype.name}, shape: {f.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m-LkTUTnpn2P"
      },
      "source": [
        "참고: CSV 텍스트의 개별 라인보다 대형 라인 배치에서 `tf.io.decode_csv`를 호출하는 것이 더 효율적입니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yp1UItJmqGqw"
      },
      "source": [
        "### `tf.data.experimental.CsvDataset`\n",
        "\n",
        "`tf.data.experimental.CsvDataset` 클래스는 `tf.data.experimental.make_csv_dataset` 함수의 편리한 특성인 열 헤더 파싱, 열 유형 추론, 자동 셔플링, 파일 인터리빙 없이 최소한의 CSV `Dataset` 인터페이스를 제공합니다.\n",
        "\n",
        "이 생성자는 `tf.io.decode_csv`와 같은 방식으로 `record_defaults`를 사용합니다.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9OzZLp3krP-t"
      },
      "outputs": [],
      "source": [
        "simple_titanic = tf.data.experimental.CsvDataset(titanic_file_path, record_defaults=titanic_types, header=True)\n",
        "\n",
        "for example in simple_titanic.take(1):\n",
        "  print([e.numpy() for e in example])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_HBmfI-Ks7dw"
      },
      "source": [
        "위의 코드는 기본적으로 다음과 같습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E5O5d69Yq7gG"
      },
      "outputs": [],
      "source": [
        "def decode_titanic_line(line):\n",
        "  return tf.io.decode_csv(line, titanic_types)\n",
        "\n",
        "manual_titanic = (\n",
        "    # Load the lines of text\n",
        "    tf.data.TextLineDataset(titanic_file_path)\n",
        "    # Skip the header row.\n",
        "    .skip(1)\n",
        "    # Decode the line.\n",
        "    .map(decode_titanic_line)\n",
        ")\n",
        "\n",
        "for example in manual_titanic.take(1):\n",
        "  print([e.numpy() for e in example])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5R3ralsnt2AC"
      },
      "source": [
        "#### 여러 파일\n",
        "\n",
        "`tf.data.experimental.CsvDataset`을 사용하여 글꼴 데이터세트를 파싱하려면 먼저 `record_defaults`에 대한 열 유형을 결정해야 합니다. 우선 한 파일의 첫 번째 행을 검사합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3tlFOTjCvAI5"
      },
      "outputs": [],
      "source": [
        "font_line = pathlib.Path(font_csvs[0]).read_text().splitlines()[1]\n",
        "print(font_line)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "etyGu8K_ySRz"
      },
      "source": [
        "처음 두 개의 필드만 문자열이고 나머지는 정수 또는 부동 소수점이며 쉼표를 계산하여 총 특성 수를 얻을 수 있습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "crgZZn0BzkSB"
      },
      "outputs": [],
      "source": [
        "num_font_features = font_line.count(',')+1\n",
        "font_column_types = [str(), str()] + [float()]*(num_font_features-2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YeK2Pw540RNj"
      },
      "source": [
        "`tf.data.experimental.CsvDataset` 생성자는 입력 파일 목록을 가져올 수 있지만 순차적으로 읽습니다. CSV 목록의 첫 번째 파일은 `AGENCY.csv`입니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_SvL5Uvl0r0N"
      },
      "outputs": [],
      "source": [
        "font_csvs[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EfAX3G8Xywy6"
      },
      "source": [
        "따라서 파일 목록을 `CsvDataset`에 전달할 때 `AGENCY.csv`의 레코드를 먼저 읽습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gtr1E66VmBqj"
      },
      "outputs": [],
      "source": [
        "simple_font_ds = tf.data.experimental.CsvDataset(\n",
        "    font_csvs,\n",
        "    record_defaults=font_column_types,\n",
        "    header=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k750Mgq4yt_o"
      },
      "outputs": [],
      "source": [
        "for row in simple_font_ds.take(10):\n",
        "  print(row[0].numpy())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NiqWKQV21FrE"
      },
      "source": [
        "여러 파일을 인터리브하려면 `Dataset.interleave`를 사용합니다.\n",
        "\n",
        "CSV 파일 이름이 포함된 초기 데이터세트는 다음과 같습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t9dS3SNb23W8"
      },
      "outputs": [],
      "source": [
        "font_files = tf.data.Dataset.list_files(\"fonts/*.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TNiLHMXpzHy5"
      },
      "source": [
        "이렇게 하면 각 epoch마다 파일 이름을 셔플합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zNd-TYyNzIgg"
      },
      "outputs": [],
      "source": [
        "print('Epoch 1:')\n",
        "for f in list(font_files)[:5]:\n",
        "  print(\"    \", f.numpy())\n",
        "print('    ...')\n",
        "print()\n",
        "\n",
        "print('Epoch 2:')\n",
        "for f in list(font_files)[:5]:\n",
        "  print(\"    \", f.numpy())\n",
        "print('    ...')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B0QB1PtU3WAN"
      },
      "source": [
        "`interleave` 메서드는 상위 `Dataset`의 각 요소마다 하위 `Dataset`를 생성하는 `map_func`를 사용합니다.\n",
        "\n",
        "여기에서 파일 데이터세트의 각 요소에서 `tf.data.experimental.CsvDataset`을 생성하려고 합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QWp4rH0Q4uPh"
      },
      "outputs": [],
      "source": [
        "def make_font_csv_ds(path):\n",
        "  return tf.data.experimental.CsvDataset(\n",
        "    path,\n",
        "    record_defaults=font_column_types,\n",
        "    header=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VxRGdLMB5nRF"
      },
      "source": [
        "인터리브로 반환한 `Dataset`는 여러 하위 `Dataset`를 순환하며 요소를 반환합니다. 아래에서 데이터세트가 `cycle_length=3` 세 가지 글꼴 파일을 순환하는 방식을 확인하세요."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OePMNF_x1_Cc"
      },
      "outputs": [],
      "source": [
        "font_rows = font_files.interleave(make_font_csv_ds,\n",
        "                                  cycle_length=3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UORIGWLy54-E"
      },
      "outputs": [],
      "source": [
        "fonts_dict = {'font_name':[], 'character':[]}\n",
        "\n",
        "for row in font_rows.take(10):\n",
        "  fonts_dict['font_name'].append(row[0].numpy().decode())\n",
        "  fonts_dict['character'].append(chr(row[2].numpy()))\n",
        "\n",
        "pd.DataFrame(fonts_dict)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mkKZa_HX8zAm"
      },
      "source": [
        "#### 공연\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8BtGHraUApdJ"
      },
      "source": [
        "이전에 `tf.io.decode_csv`가 문자열 배치에서 실행될 때 더 효율적이라는 점을 언급했습니다.\n",
        "\n",
        "대형 배치 크기를 사용할 때 이 사실을 활용하여 CSV 로드 성능을 향상시킬 수 있습니다(단, 먼저 [캐싱](#caching)을 시도해야 함)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d35zWMH7MDL1"
      },
      "source": [
        "내장 로더 20을 사용할 경우 2048개의 예제 배치에 약 17초가 걸립니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ieUVAPryjpJS"
      },
      "outputs": [],
      "source": [
        "BATCH_SIZE=2048\n",
        "fonts_ds = tf.data.experimental.make_csv_dataset(\n",
        "    file_pattern = \"fonts/*.csv\",\n",
        "    batch_size=BATCH_SIZE, num_epochs=1,\n",
        "    num_parallel_reads=100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MUC2KW4LkQIz"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "for i,batch in enumerate(fonts_ds.take(20)):\n",
        "  print('.',end='')\n",
        "\n",
        "print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5lhnh6rZEDS2"
      },
      "source": [
        "**텍스트 줄 배치**를 `decode_csv`에 전달하면 더 빠르게 약 5초 만에 실행됩니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4XbPZV1okVF9"
      },
      "outputs": [],
      "source": [
        "fonts_files = tf.data.Dataset.list_files(\"fonts/*.csv\")\n",
        "fonts_lines = fonts_files.interleave(\n",
        "    lambda fname:tf.data.TextLineDataset(fname).skip(1),\n",
        "    cycle_length=100).batch(BATCH_SIZE)\n",
        "\n",
        "fonts_fast = fonts_lines.map(lambda x: tf.io.decode_csv(x, record_defaults=font_column_types))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "te9C2km-qO8W"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "for i,batch in enumerate(fonts_fast.take(20)):\n",
        "  print('.',end='')\n",
        "\n",
        "print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aebC1plsMeOi"
      },
      "source": [
        "대규모 배치를 사용하여 CSV 성능을 높이는 또 다른 예는 [과대적합과 과소적합 튜토리얼](../keras/overfit_and_underfit.ipynb)을 참조하세요.\n",
        "\n",
        "이러한 종류의 접근 방식이 효과가 있을 수 있지만 `Dataset.cache` 및 `tf.data.Dataset.snapshot`과 같은 다른 옵션과 함께 데이터를 보다 간소화된 형식으로 다시 인코딩하는 방법도 고려하세요."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "csv.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
